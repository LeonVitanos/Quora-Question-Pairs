{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anakthsh",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B5y98tPGPCt",
        "colab_type": "code",
        "outputId": "66292c65-0e58-4d2b-df0a-2b37d6469168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTrjz19VETdg",
        "colab_type": "code",
        "outputId": "0865b644-041d-4f70-afae-957ef034563b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "WNL = WordNetLemmatizer()\n",
        "STOP_WORDS = stopwords.words(\"english\")\n",
        "REMOVE_STOPWORDS=True\n",
        "STEM_WORDS=True\n",
        "LEM_INSTEAD_OF_STEM=True\n",
        "\n",
        "def cutter(word):\n",
        "    if len(word) < 4:\n",
        "        return word\n",
        "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
        "\n",
        "def preprocess(string):\n",
        "    # Convert words to lower case and clean the text\n",
        "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
        "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
        "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
        "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
        "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
        "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
        "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
        "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
        "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
        "\n",
        "    # Remove punctuation from text\n",
        "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if REMOVE_STOPWORDS:\n",
        "        string = string.split()\n",
        "        string = [w for w in string if not w in STOP_WORDS]\n",
        "        string = \" \".join(string)\n",
        "\n",
        "    # Optionally, shorten words to their stems or lemmatize\n",
        "    if STEM_WORDS:\n",
        "        if LEM_INSTEAD_OF_STEM:\n",
        "          string = ' '.join([cutter(w) for w in string.split()])\n",
        "        else:\n",
        "          string = string.split()\n",
        "          stemmer = SnowballStemmer('english')\n",
        "          stemmed_words = [stemmer.stem(word) for word in string]\n",
        "          string = \" \".join(stemmed_words)\n",
        "\n",
        "    return string\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv(\"train_original.csv\")\n",
        "unprocessed_qid1=train[\"question1\"].copy()\n",
        "unprocessed_qid2=train[\"question2\"].copy()\n",
        "\n",
        "print(\"Clearing text...\")\n",
        "train[\"question1\"] = train[\"question1\"].fillna(\"\").apply(preprocess)\n",
        "train[\"question2\"] = train[\"question2\"].fillna(\"\").apply(preprocess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Clearing text...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-8coUOSmlfA",
        "colab_type": "code",
        "outputId": "787fcae8-1708-4026-8692-14fe441811bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import random\n",
        "rn=random.randint(0, 100)\n",
        "print(\"For example, the question \\n\",unprocessed_qid1[rn], \"\\nbecame \\n\", train[\"question1\"][rn], \"\",sep='')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For example, the question \n",
            "Can I make 50,000 a month by day trading?\n",
            "became \n",
            "make 50k month day trade\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf6WPrUFy7Qa",
        "colab_type": "code",
        "outputId": "b6211ce2-d626-4bd2-b54b-b8d82c1bd98b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#FOR PART A\n",
        "\n",
        "def common_words_ratio(q1,q2):\n",
        "  q1s = q1.split()\n",
        "  q2s = q2.split()\n",
        "  \n",
        "  avg_length = (len(q1s)+len(q2s))/2\n",
        "  if avg_length==0:\n",
        "    return 0\n",
        "  common_count=0\n",
        "  for word in q1s:\n",
        "    for wordy in q2s:\n",
        "      if word==wordy:\n",
        "        common_count=common_count+1\n",
        "        break\n",
        "        \n",
        "  return common_count/avg_length\n",
        "\n",
        "print(\"Making list of common words between 2 questions...\")\n",
        "common_words=[]\n",
        "for index, row in train.iterrows():\n",
        "  common_words.append(common_words_ratio(row[\"question1\"], row[\"question2\"]))\n",
        "\n",
        "print(\"Training model...\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "common_words = np.array(common_words).reshape(-1, 1)\n",
        "labels = np.array(train[\"is_duplicate\"])\n",
        "X_train, X_test, y_train, y_test = train_test_split(common_words, labels, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=30, random_state=21)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy on test set: {:.3f}\".format(model.score(X_test, y_test)))\n",
        "\n",
        "pred = model.predict_proba(X_test)\n",
        "metric_1 = log_loss(y_test, pred)\n",
        "print (\"Logloss on test set: {:.3f}\".format(metric_1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making list of common words between 2 questions...\n",
            "Training model...\n",
            "Accuracy on test set: 0.679\n",
            "Logloss on test set: 0.536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zEWNDG8fSP",
        "colab_type": "code",
        "outputId": "2b2eaecd-4bc6-4ac8-dbf7-17cfff7eacb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#FOR PART B\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "corpus=pd.concat([train[\"question1\"],train[\"question2\"]])\n",
        "cv = CountVectorizer().fit(corpus)\n",
        "\n",
        "print(\"Calculating cosine similarities between questions...\")\n",
        "cosine_similarities=[]\n",
        "size=train.shape[0]\n",
        "for j in range(size):\n",
        "    a = cv.transform([train.iloc[j].question1])\n",
        "    b = cv.transform([train.iloc[j].question2])\n",
        "    cosine_similarities.append(cosine_similarity(a,b).ravel()[0])\n",
        "    \n",
        "border=0.50\n",
        "print(\"Calculating accuracy for border\",border,\"...\")\n",
        "#Labels are only used to find accuracy\n",
        "correct_predictions=0\n",
        "for j in range(size):\n",
        "  if (cosine_similarities[j]>=border and train.iloc[j].is_duplicate==1) or (cosine_similarities[j]<border and train.iloc[j].is_duplicate==0):\n",
        "    correct_predictions=correct_predictions+1\n",
        "print(\"For border\",border,\"accuracy is: {:.4f}\".format(correct_predictions/size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating cosine similarities between questions...\n",
            "Calculating accuracy for border 0.5 ...\n",
            "For border 0.5 accuracy is: 0.6348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bacMhIGrMQR",
        "colab_type": "code",
        "outputId": "8703c028-5982-49f9-9ff4-a1bf048f78c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#FOR PART C\n",
        "\n",
        "print(\"Creating list of unique questions...\")\n",
        "list_of_unique_questions = [0] * 537934 #537933 unique questions\n",
        "\n",
        "n_of_pairs=len(train)\n",
        "for i in range(n_of_pairs):\n",
        "  \n",
        "  if list_of_unique_questions[train[\"qid1\"][i]]==0:\n",
        "    list_of_unique_questions[train[\"qid1\"][i]]=train[\"question1\"][i]\n",
        "    \n",
        "  if list_of_unique_questions[train[\"qid2\"][i]]==0:\n",
        "    list_of_unique_questions[train[\"qid2\"][i]]=train[\"question2\"][i]\n",
        "    \n",
        "print(\"Creating list of unique unprocessed questions...\")\n",
        "list_of_unique_unprocessed_questions = [0] * 537934 #537933 unique questions\n",
        "\n",
        "n_of_pairs=len(train)\n",
        "for i in range(n_of_pairs):\n",
        "  \n",
        "  if list_of_unique_unprocessed_questions[train[\"qid1\"][i]]==0:\n",
        "    list_of_unique_unprocessed_questions[train[\"qid1\"][i]]=unprocessed_qid1[i]\n",
        "    \n",
        "  if list_of_unique_unprocessed_questions[train[\"qid2\"][i]]==0:\n",
        "    list_of_unique_unprocessed_questions[train[\"qid2\"][i]]=unprocessed_qid2[i]\n",
        "    \n",
        "print(\"Creating dictionary of words (inverted index)...\")\n",
        "inverted = {}\n",
        "for i in range(1,len(list_of_unique_questions)):\n",
        "  words = list_of_unique_questions[i].split()    \n",
        "  for word in words:\n",
        "    inverted.setdefault(word, [])\n",
        "    if i not in inverted[word]:\n",
        "      inverted[word].append(i)\n",
        "\n",
        "#Find how many times the most common word appears\n",
        "max=0\n",
        "for word,documents in inverted.items():\n",
        "  count=len(documents)\n",
        "  if count>max:\n",
        "    max=count\n",
        "    \n",
        "#Normalization of every word, common words count less!\n",
        "for word,documents in inverted.items():\n",
        "  count=len(documents)\n",
        "  zi=(count-1)/(max)\n",
        "  documents.append(1-zi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating list of unique questions...\n",
            "Creating list of unique unprocessed questions...\n",
            "Creating dictionary of words (inverted index)...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAsO4SwwwFC",
        "colab_type": "code",
        "outputId": "ff686450-6eb2-431b-fc55-bc481cb77964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "import datetime\n",
        "a = datetime.datetime.now()\n",
        "\n",
        "#Find all documents that contain at least one word from the question\n",
        "testing_phrase=\"how to be a better programmer?\"\n",
        "print(testing_phrase)\n",
        "testing_phrase=preprocess(testing_phrase)\n",
        "words = testing_phrase.split()\n",
        "similar_docs=set()\n",
        "\n",
        "for word in words:\n",
        "  if word in inverted:\n",
        "    for doc in inverted[word][:-1]:\n",
        "      similar_docs.add(doc)\n",
        "\n",
        "#Find the most relevant documents, depending on how common a word is and how long is the question\n",
        "question_length=len(testing_phrase)\n",
        "similar_docs_points=[]\n",
        "similar_docs_texts=[]\n",
        "for doc in similar_docs:\n",
        "  words = set(list_of_unique_questions[doc].split())\n",
        "  points=0\n",
        "  for word in words:\n",
        "    if word in testing_phrase:\n",
        "      points+=inverted[word][-1]*(1/len(words))\n",
        "  similar_docs_points.append(points)\n",
        "  similar_docs_texts.append(list_of_unique_unprocessed_questions[doc])\n",
        "\n",
        "number_of_elements = 10\n",
        "Z = [x for _,x in sorted(zip(similar_docs_points,similar_docs_texts))]\n",
        "Z.reverse()\n",
        "\n",
        "b = datetime.datetime.now()\n",
        "c = b - a\n",
        "print(\"Found these questions in \", c.seconds, \".\",c.microseconds,\" seconds:\\n\", sep='')\n",
        "for x in Z[:number_of_elements]:\n",
        "   print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how to be a better programmer?\n",
            "Found these questions in 0.56920 seconds:\n",
            "\n",
            "What is a Programmer?\n",
            "How can I be a better programmer?\n",
            "What should I do to be better than better?\n",
            "How can I better myself?\n",
            "How do top programmers programm?\n",
            "What do programmers program all the time?\n",
            "Is Python better than R?\n",
            "How can I be a better programmer / developer?\n",
            "Which is better, B.A or B.Sc?\n",
            "Which Programming language is better?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}